{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4985a208",
   "metadata": {},
   "source": [
    "#### Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "6330570a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "CLASSIFICATIONS = {\n",
    "    0: 'none',\n",
    "    1: 'remove_duplicate_values',\n",
    "    2: 'fill_missing_values',\n",
    "    3: 'perform_dimensionality_reduction',\n",
    "    4: 'perform_correlation_analysis',\n",
    "}\n",
    "\n",
    "system_prompts = {f.stem: f.read_text() for f in Path(\"system_prompt\").glob(\"*.txt\")}\n",
    "system_prompt_classification = system_prompts['classification'].format(functions_dict=str(CLASSIFICATIONS))\n",
    "\n",
    "user_prompts = [f.read_text() for f in Path(\"user_prompt\").glob(\"*.txt\")]\n",
    "structured_outputs = {f.stem: json.load(f.open('r')) for f in Path(\"structured_output\").glob(\"*.json\")}\n",
    "testcases_fill_missing_values = {f.read_text() for f in Path(\"testcases/fill_missing_values\").glob(\"*.txt\")}\n",
    "testcases_remove_duplicate_values = {f.read_text() for f in Path(\"testcases/remove_duplicate_values\").glob(\"*.txt\")}\n",
    "testcases_perform_correlation_analysis = {f.read_text() for f in Path(\"testcases/perform_correlation_analysis\").glob(\"*.txt\")}\n",
    "testcases_perform_dimensionality_reduction = {f.read_text() for f in Path(\"testcases/perform_dimensionality_reduction\").glob(\"*.txt\")}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b2f57b",
   "metadata": {},
   "source": [
    "#### Main System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "ac8850a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def postprocess_classification_respond(respond):\n",
    "    match = re.search(r'\\d+', respond)\n",
    "    if match:\n",
    "        res = int(match.group(0))\n",
    "        if res < len(CLASSIFICATIONS) and res > 0:\n",
    "            return res\n",
    "    return 0\n",
    "\n",
    "def perform_classification(client, user_request):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": system_prompt_classification,\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": user_request,\n",
    "        }\n",
    "    ]\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"solar-pro\",\n",
    "        messages=messages,\n",
    "    )\n",
    "    return postprocess_classification_respond(response.choices[0].message.content)\n",
    "\n",
    "def perform_function_mapping(client, user_request, system_prompt, structured_output):\n",
    "    messages = [\n",
    "            {\n",
    "                'role': 'system',\n",
    "                'content': system_prompt\n",
    "            },\n",
    "            {\n",
    "                'role': 'user',\n",
    "                'content': user_request\n",
    "            }\n",
    "        ]\n",
    "    response = client.chat.completions.create(\n",
    "            model=\"solar-pro\",\n",
    "            messages=messages,\n",
    "            response_format=structured_output\n",
    "        )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def process_user_request(client, user_request):\n",
    "    '''\n",
    "    Outpupt: function_name (string), function_parameters (JSON), model_respond (string)\n",
    "    Return the JSON string with the information of the function that the user want to call together the respond of the system.\n",
    "    '''\n",
    "    try:\n",
    "        classification = perform_classification(client, user_request)\n",
    "        function_name = CLASSIFICATIONS[classification]\n",
    "        function_parameters = {}\n",
    "        \n",
    "        if classification != 0:\n",
    "            function_parameters = perform_function_mapping(\n",
    "                client, user_request, \n",
    "                system_prompt = system_prompts[CLASSIFICATIONS[classification]], \n",
    "                structured_output = structured_outputs[CLASSIFICATIONS[classification]])\n",
    "            function_parameters = json.loads(function_parameters)\n",
    "        return True, function_name, function_parameters\n",
    "    except Exception as e:\n",
    "        return False, e, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "5b29d01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import io\n",
    "import base64\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def check_cols_in_dataframe(df: pd.DataFrame, cols):\n",
    "    # Edge Cases\n",
    "    if cols is None or cols == []:\n",
    "        return True\n",
    "    df_columns = df.columns\n",
    "    \n",
    "    for col in cols:\n",
    "        if col not in df_columns:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def convert_subset_to_message(subset):\n",
    "    return subset if subset not in [None, []] else 'all'\n",
    "\n",
    "'''\n",
    "The funcitons below return success (bool), message (str), processed_data (pandas.DataFrame), image\n",
    "'''\n",
    "def remove_duplicate_values(df: pd.DataFrame, subset, keep):\n",
    "    if keep not in ['first', 'last', 'false']:\n",
    "        return False, f'Keep is not recognized: {keep}', None, None\n",
    "    \n",
    "    if check_cols_in_dataframe(df,subset):\n",
    "        try:\n",
    "            return_message = f'Here is your data with duplicate values remove in subset: {convert_subset_to_message(subset)}, keep: {keep}'\n",
    "            # String to boolean\n",
    "            if keep == 'false':\n",
    "                keep = False\n",
    "            \n",
    "            # Subset not specified\n",
    "            if subset == []:\n",
    "                subset = None\n",
    "            \n",
    "            return True, return_message, df.drop_duplicates(subset=subset, keep=keep), None\n",
    "        except Exception as e:\n",
    "            return False, e, None, None\n",
    "    else:\n",
    "        return False, f'Columns not in the DataFrame: {subset}', None, None\n",
    "\n",
    "\n",
    "def fill_missing_values(df: pd.DataFrame, subset, metric):\n",
    "    if metric not in ['mean', 'median', 'mode']:\n",
    "        return False, f'Metric is not recognized: {metric}', None, None\n",
    "    \n",
    "    if check_cols_in_dataframe(df, subset):\n",
    "        try:\n",
    "            df_result = df.copy()\n",
    "            return_message = f'Here is your data with missing values remove in subset: {convert_subset_to_message(subset)}, metric: {metric}'\n",
    "            # subset not specified\n",
    "            if subset == None or subset == []:\n",
    "                subset = df.select_dtypes(include=np.number).columns.tolist()\n",
    "            \n",
    "            for col in subset:\n",
    "                if metric == 'mean':\n",
    "                    fill_value = df_result[col].mean()\n",
    "                elif metric == 'median':\n",
    "                    fill_value = df_result[col].median()\n",
    "                elif metric == 'mode':\n",
    "                    fill_value = df_result[col].mode()\n",
    "                df_result[col] = df_result[col].fillna(fill_value)\n",
    "            return True, return_message, df_result, None\n",
    "        except Exception as e:\n",
    "            return False, e, None, None\n",
    "    else:\n",
    "        return False, f'Columns not in the DataFrame: {subset}', None, None\n",
    "    \n",
    "def perform_correlation_analysis(df: pd.DataFrame, subset, method):\n",
    "    if method not in ['pearson', 'spearman']:\n",
    "        return False, f\"Method is not recognized: {method}\", None, None\n",
    "    \n",
    "    if check_cols_in_dataframe(df, subset):\n",
    "        try:\n",
    "            if subset == None or subset == []:\n",
    "                subset = df.select_dtypes(include=np.number).columns.tolist()\n",
    "            \n",
    "            corr = df[subset].corr(method=method)\n",
    "            buf = io.BytesIO()\n",
    "            plt.figure()\n",
    "            annot_curr = True if len(subset) < 10 else False\n",
    "            sns.heatmap(corr, annot=annot_curr, cmap='coolwarm', fmt='.2f', square=True, cbar_kws={\"shrink\": .8})\n",
    "            plt.title('Correlation Heatmap')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(buf, format='png')\n",
    "            plt.close()\n",
    "            buf.seek(0)\n",
    "            img_base64 = base64.b64encode(buf.read()).decode('utf-8')\n",
    "            return True, f'Here is the correlation analysis in subset: {convert_subset_to_message(subset)}', corr, img_base64\n",
    "        except Exception as e:\n",
    "            return False, e, None, None\n",
    "    else:\n",
    "        return False, f'Columns not in the DataFrame: {subset}', None, None\n",
    "    \n",
    "def perform_dimensionality_reduction(df: pd.DataFrame, features, target):\n",
    "    subset = features + [target]\n",
    "    \n",
    "    if check_cols_in_dataframe(df, subset):\n",
    "        try:\n",
    "            if features == None or features == []:\n",
    "                features = df.select_dtypes(include=np.number).columns.tolist()\n",
    "            \n",
    "            return_message = f'Here is the pca result of features: {features}, target: {target}'\n",
    "            # PCA\n",
    "            X = df[features]\n",
    "            y = df[target]\n",
    "            pca = PCA(n_components=2)\n",
    "            X_pca = pca.fit_transform(X)\n",
    "            pca_df = pd.DataFrame(X_pca, columns=['PC1', 'PC2'], index=df.index)\n",
    "            \n",
    "            # Plot\n",
    "            plt.figure(figsize=(8,6))\n",
    "            sns.scatterplot(x='PC1', y='PC2', hue=y, palette='viridis', data=pca_df.join(y))\n",
    "            plt.title('PCA Scatter Plot (PC1 vs PC2)')\n",
    "            plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.2f}% variance)')\n",
    "            plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.2f}% variance)')\n",
    "            plt.legend(title=target)\n",
    "            \n",
    "            buf = io.BytesIO()\n",
    "            plt.savefig(buf, format='png')\n",
    "            plt.close()\n",
    "            buf.seek(0)\n",
    "            img_base64 = base64.b64encode(buf.read()).decode('utf-8')\n",
    "            \n",
    "            pca_params = {\n",
    "                'explained_variance': pca.explained_variance_,\n",
    "                'explained_variance_ratio': pca.explained_variance_.tolist(),\n",
    "                'components': pca.components_.tolist()\n",
    "            }\n",
    "            return True, return_message, pca_params, img_base64 \n",
    "        except Exception as e:\n",
    "            return False, e, None, None\n",
    "    else:\n",
    "        return False, f'Columns not in the DataFrame: {subset}', None, None\n",
    "        \n",
    "def map_json_to_function(df: pd.DataFrame, function_name, function_parameters):\n",
    "    if function_name == 'remove_duplicate_values':\n",
    "        return remove_duplicate_values(df, subset=function_parameters['subset'], keep=function_parameters['keep'])\n",
    "    elif function_name == 'fill_missing_values':\n",
    "        return fill_missing_values(df, subset=function_parameters['subset'], metric=function_parameters['metric'])\n",
    "    elif function_name == 'perform_correlation_analysis':\n",
    "        return perform_correlation_analysis(df, subset=function_parameters['subset'], method=function_parameters['method'])\n",
    "    elif function_name == 'perform_dimensionality_reduction':\n",
    "        return perform_dimensionality_reduction(df, features=function_parameters['features'], target=function_parameters['target'])\n",
    "    else:\n",
    "        return False, f'No function recognized.', None, None\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e171a9c",
   "metadata": {},
   "source": [
    "### Testing\n",
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "62c9c4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv(\"../../data/train.csv\")\n",
    "test_df = pd.read_csv(\"../../data/test.csv\")\n",
    "modified_train_df = pd.read_csv(\"../../data/modified_train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2794af90",
   "metadata": {},
   "source": [
    "#### Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "7bfe880c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import toml\n",
    "\n",
    "parsed_toml = toml.load('../../../.streamlit/secrets.toml')\n",
    "# API_KEY = st.secrets.get('UPSTAGE_API_KEY', None)\n",
    "# parsed_toml = toml.load('../../../secrets.toml')\n",
    "\n",
    "client = OpenAI(\n",
    "\t# api_key=parsed_toml['UPSTAGE_API_KEY'], \n",
    "    # api_key=API_KEY,\n",
    "    api_key=parsed_toml['upstage_api_key_v2'],\n",
    " \tbase_url=\"https://api.upstage.ai/v1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad4eed4",
   "metadata": {},
   "source": [
    "#### Remove Duplicate Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "32df7be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_prompt: Please remove duplicated value in the weather column, keep the last sample of the value.\n",
      "function_name: remove_duplicate_values\n",
      "function_parameters: {'keep': 'last', 'subset': ['weather']}\n",
      "Here is your data with duplicate values remove in subset: ['weather'], keep: last\n",
      "\n",
      "user_prompt: Please remove duplication in the dataset, keep the first appearance of the value.\n",
      "function_name: remove_duplicate_values\n",
      "function_parameters: {'keep': 'first', 'subset': []}\n",
      "Here is your data with duplicate values remove in subset: all, keep: first\n",
      "\n",
      "user_prompt: Remove duplication values in columns temp, atemp, and humidity. Don't keep any duplication values.\n",
      "function_name: remove_duplicate_values\n",
      "function_parameters: {'keep': 'false', 'subset': ['temp', 'atemp', 'humidity']}\n",
      "Here is your data with duplicate values remove in subset: ['temp', 'atemp', 'humidity'], keep: false\n",
      "\n"
     ]
    }
   ],
   "source": [
    "outputs = []\n",
    "\n",
    "for user_prompt in testcases_remove_duplicate_values:\n",
    "    print(f\"user_prompt: {user_prompt}\")\n",
    "    success_function_info, function_name, function_parameters = process_user_request(client, user_request=user_prompt)\n",
    "    print(f\"function_name: {function_name}\")\n",
    "    print(f\"function_parameters: {function_parameters}\")\n",
    "    if success_function_info:\n",
    "        success_request, message, output, image = map_json_to_function(modified_train_df, function_name, function_parameters)\n",
    "        outputs.append(output)\n",
    "        print(message)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "18809ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>season</th>\n",
       "      <th>holiday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weather</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>humidity</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>casual</th>\n",
       "      <th>registered</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-01-01 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.84</td>\n",
       "      <td>14.395</td>\n",
       "      <td>81</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011-01-01 01:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.02</td>\n",
       "      <td>13.635</td>\n",
       "      <td>80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-01-01 02:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.02</td>\n",
       "      <td>13.635</td>\n",
       "      <td>80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>27</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-01-01 03:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.84</td>\n",
       "      <td>14.395</td>\n",
       "      <td>75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011-01-01 04:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.84</td>\n",
       "      <td>14.395</td>\n",
       "      <td>75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              datetime  season  holiday  workingday  weather  temp   atemp  \\\n",
       "0  2011-01-01 00:00:00       1        0           0        1  9.84  14.395   \n",
       "1  2011-01-01 01:00:00       1        0           0        1  9.02  13.635   \n",
       "2  2011-01-01 02:00:00       1        0           0        1  9.02  13.635   \n",
       "3  2011-01-01 03:00:00       1        0           0        1  9.84  14.395   \n",
       "4  2011-01-01 04:00:00       1        0           0        1  9.84  14.395   \n",
       "\n",
       "   humidity  windspeed  casual  registered  count  \n",
       "0        81        0.0       3          13     16  \n",
       "1        80        0.0       8          32     40  \n",
       "2        80        0.0       5          27     32  \n",
       "3        75        0.0       3          10     13  \n",
       "4        75        0.0       0           1      1  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id = 0\n",
    "subset = outputs[id].columns\n",
    "\n",
    "duplications = outputs[id][subset].duplicated()\n",
    "print(len(duplications[duplications == True]))\n",
    "outputs[id].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1737ed58",
   "metadata": {},
   "source": [
    "#### Fill Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "272ef236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_prompt: Fill the missing values with mean.\n",
      "function_name: fill_missing_values\n",
      "function_parameters: {'metric': 'mean', 'subset': []}\n",
      "Here is your data with missing values remove in subset: all, metric: mean\n",
      "\n",
      "user_prompt: Fill the NA values with the column's most frequent value in 'windspeed'.\n",
      "function_name: Error code: 500 - {'error': {'message': 'internal_server_error', 'type': 'internal_server_error', 'param': None, 'code': None}}\n",
      "function_parameters: {}\n",
      "\n",
      "user_prompt: Fill the NULL values with median in class 'temp' and 'atemp'.\n",
      "function_name: Error code: 500 - {'error': {'message': 'internal_server_error', 'type': 'internal_server_error', 'param': None, 'code': None}}\n",
      "function_parameters: {}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "outputs = []\n",
    "\n",
    "for user_prompt in testcases_fill_missing_values:\n",
    "    print(f\"user_prompt: {user_prompt}\")\n",
    "    success_function_info, function_name, function_parameters = process_user_request(client, user_request=user_prompt)\n",
    "    print(f\"function_name: {function_name}\")\n",
    "    print(f\"function_parameters: {function_parameters}\")\n",
    "    if success_function_info:\n",
    "        success_request, message, output, image = map_json_to_function(modified_train_df, function_name, function_parameters)\n",
    "        outputs.append(output)\n",
    "        print(message)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "eb42c8fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>season</th>\n",
       "      <th>holiday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weather</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>humidity</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>casual</th>\n",
       "      <th>registered</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [datetime, season, holiday, workingday, weather, temp, atemp, humidity, windspeed, casual, registered, count]\n",
       "Index: []"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id = 0\n",
    "rows_with_nan = outputs[id].loc[outputs[id].isna().any(axis=1)]\n",
    "rows_with_nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979e37a8",
   "metadata": {},
   "source": [
    "#### Perform Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "273f42a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_prompt: Give me the heatmap of the entire database with standard pearson correlation coefficient.\n",
      "function_name: perform_correlation_analysis\n",
      "function_parameters: {'method': 'pearson', 'subset': []}\n",
      "Here is the correlation analysis in subset: ['season', 'holiday', 'workingday', 'weather', 'temp', 'atemp', 'humidity', 'windspeed', 'casual', 'registered', 'count']\n",
      "\n",
      "user_prompt: Execute correlation analysis on columns weather, atemp, temp, and humidity with the metric spearman rank correlation.\n",
      "function_name: perform_correlation_analysis\n",
      "function_parameters: {'method': 'spearman', 'subset': ['weather', 'atemp', 'temp', 'humidity']}\n",
      "Here is the correlation analysis in subset: ['weather', 'atemp', 'temp', 'humidity']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "outputs = []\n",
    "images = []\n",
    "\n",
    "for user_prompt in testcases_perform_correlation_analysis:\n",
    "    print(f\"user_prompt: {user_prompt}\")\n",
    "    success_function_info, function_name, function_parameters = process_user_request(client, user_request=user_prompt)\n",
    "    print(f\"function_name: {function_name}\")\n",
    "    print(f\"function_parameters: {function_parameters}\")\n",
    "    if success_function_info:\n",
    "        success_request, message, output, image = map_json_to_function(train_df, function_name, function_parameters)\n",
    "        outputs.append(output)\n",
    "        images.append(image)\n",
    "        print(message)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "838785a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           weather     atemp      temp  humidity\n",
      "weather   1.000000 -0.061933 -0.057912  0.399492\n",
      "atemp    -0.061933  1.000000  0.987128 -0.042028\n",
      "temp     -0.057912  0.987128  1.000000 -0.046854\n",
      "humidity  0.399492 -0.042028 -0.046854  1.000000\n"
     ]
    }
   ],
   "source": [
    "id = 1\n",
    "\n",
    "print(outputs[id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "43a91d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "for id, image in enumerate(images):\n",
    "    img = Image.open(io.BytesIO(base64.decodebytes(bytes(image, \"utf-8\"))))\n",
    "    if img.mode in (\"RGBA\", \"P\"):\n",
    "        img = img.convert(\"RGB\")\n",
    "    img.save(f'../../data/corr_{id}.jpeg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79c261c",
   "metadata": {},
   "source": [
    "#### Perform Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "8f6549d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_prompt: I want you to perform PCA with the target column weather.\n",
      "function_name: perform_dimensionality_reduction\n",
      "function_parameters: {'features': [], 'target': 'weather'}\n",
      "Here is the pca result of features: ['season', 'holiday', 'workingday', 'weather', 'temp', 'atemp', 'humidity', 'windspeed', 'casual', 'registered', 'count'], target: weather\n",
      "\n",
      "user_prompt: I want you to excecute dimensionality reduction with target colum: season with features: temp, atemp, humidity, and windspeed.\n",
      "\n",
      "function_name: Error code: 500 - {'error': {'message': 'internal_server_error', 'type': 'internal_server_error', 'param': None, 'code': None}}\n",
      "function_parameters: {}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "outputs = []\n",
    "images = []\n",
    "\n",
    "for user_prompt in testcases_perform_dimensionality_reduction:\n",
    "    print(f\"user_prompt: {user_prompt}\")\n",
    "    success_function_info, function_name, function_parameters = process_user_request(client, user_request=user_prompt)\n",
    "    print(f\"function_name: {function_name}\")\n",
    "    print(f\"function_parameters: {function_parameters}\")\n",
    "    if success_function_info:\n",
    "        success_request, message, output, image = map_json_to_function(train_df, function_name, function_parameters)\n",
    "        outputs.append(output)\n",
    "        images.append(image)\n",
    "        print(message)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "01927cb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'explained_variance': array([55871.57393504,  2333.22480586]),\n",
       " 'explained_variance_ratio': [55871.573935036846, 2333.2248058608875],\n",
       " 'components': [[0.0007759861504612084,\n",
       "   -7.302777989881548e-06,\n",
       "   9.05256815162328e-05,\n",
       "   -0.00033275700486311803,\n",
       "   0.0123123913871312,\n",
       "   0.013230045725711497,\n",
       "   -0.024810042883131784,\n",
       "   0.00343235079884392,\n",
       "   0.1340059051060151,\n",
       "   0.6301731215536966,\n",
       "   0.7641790266597118],\n",
       "  [-0.0003547572485204797,\n",
       "   0.000221851890345421,\n",
       "   -0.004285006755954742,\n",
       "   -0.0011459370560865797,\n",
       "   0.04953853973057426,\n",
       "   0.053258703294930626,\n",
       "   -0.09239960857314228,\n",
       "   0.008201758763864439,\n",
       "   0.7992977644486559,\n",
       "   -0.5174819488671101,\n",
       "   0.28181581558154595]]}"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "28c268e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "for id, image in enumerate(images):\n",
    "    img = Image.open(io.BytesIO(base64.decodebytes(bytes(image, \"utf-8\"))))\n",
    "    if img.mode in (\"RGBA\", \"P\"):\n",
    "        img = img.convert(\"RGB\")\n",
    "    img.save(f'../../data/pca_{id}.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb84c6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
